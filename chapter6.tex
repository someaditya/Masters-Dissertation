\chapter{Conclusion}
Chapter Six concludes the dissertation. Its motivation is to show that the research objectives depicted in Chapter One have been met, to express the research contribution this work represents, and to discuss the future work prospects.

\section{Objective Assessment}
\subsection{Creation of English to Hindi Neural Translation Model}
\textbf{Research Objective} \textit{To create a baseline neural translation model for English to Hindi translation which has the ability to generate above average quality translations by using a large parallel corpus.
}

The primary objective of this dissertation was to choose a large English-Hindi Parallel Corpus and create a neural translation model which can be used as a baseline model for English to Hindi translations. The initial step in training of quality neural translations model require large parallel corpus. This was addressed by selecting the largest publicly available Hindi-English Corpus which is the IIT Bombay English-Hindi Corpus. 

Then in the next phase, the English to Hindi neural translation model was trained using the training data from the IIT Bombay English-Hindi Corpus. Several different models with different configurations based on previous researches were experimented and finally the best performing model was selected as the baseline model. The baseline model obtained an above average BLEU score during evaluation which was an adequate step towards addressing the research question.

\subsection{Utilizing a Hindi Monolingual Corpus for Re-training}
\textbf{Research Objective} \textit{To Fine Tune (Re-Train) the neural translation model by utilizing  a  Hindi Monolingual Corpus.}

The secondary objective of this research was to utilize a Hindi monolingual corpus for fine-tuning the baseline neural translation model. A technical approach was needed which allowed the use of monolingual corpus in neural translation models. After a brief study of different approaches, the Back Translation was selected for utilizing a monolingual corpus for fine-tuning the baseline model.

For this work , the IIT-Bombay Monolingual Corpus was initially used for generating synthetic English sentences for human generated Hindi sentences. A reverse Hindi to English neural translation model was trained and the sentences from the monolingual corpus were translated using the same model. The machine generated synthetic English sentences along with the original Hindi sentences were used to re-train the baseline English to Hindi translation model. The results showed a marginal increment in the BLEU scores which was probably due to the less number of synthetic data considered for the experiment.

\subsection{Creation of Domain Specific Corpus}
\textbf{Research Objective} \textit{To create a new Domain Specific Corpus for the Tourism Domain.}

To address the research question there was a need to create a new domain specific corpus for the tourism domain. The domain specific corpus was designed to contain cross-domain specific training data and domain-specific test data. To create the cross-domain specific training data , the human translated transcripts from TED Talks website were used which ensure gold quality data. As the corpus of domain specific test data was created using the sentences from tourism blogs , the cross-domain training corpus based on TED Talks Website was justified as both the mediums, Tripoto travel blogs and TED Talks contains similar kind of texts, that is experiences of people which are generally in same form of speech. 

The training corpus that was generated from TED talks was of good quality and it provided an adequate foundation for the research question to be addressed.  

\subsection{Domain specific Fine-Tuning}
\textbf{Research Objective} \textit{To Fine Tune (Re-Train) the neural translation model with Cross-Domain Specific Training Data.
}

In the pursuit of research's goal, the domain specific fine-tuning (re-training) of the neural translation model with cross-domain specific training data was a major aim. The pre-trained neural model (baseline with back-translated training) was re-trained using the cross-domain specific corpus that was created as part of this research.In the fine-tuning process , the neural model was trained with a new corpus which helped it to learn new words which was significant in domain adaptation. The corpus contained around 81K sentence pairs, and the re-training showed a considerable improvement of BLEU scores on the original test data. 


\subsection{Final Remarks }
\textbf{Research Objective} \textit{To Test and Evaluate the neural translation model with Domain Specific Test Data from the Domain Specific Corpus.}

The significant result of this research has been a demonstration of the potential that exists inside the study of neural machine translation for English-Hindi language pairs, which implies that the models trained with domain specific data can obtain better quality translations than other baseline models.

At the last phase of evaluation process, the three models baseline, baseline with back translation and domain trained model were tested and evaluated with the domain specific test data. The results shows that domain specific training of a neural translation model with cross-domain specific training data leads to domain adaptation. The domain specific fine-tuning data, which consisted transcripts from TED Talks, was used to fine-tune the translation model. It was tested with the test data consisting of sentences extracted from travel blogs. The results of the experiments indicated that a cross-domain trained neural translation model performs well on the in-domain data, and subsequently generates above average quality translations. 

\section{Research Contribution}
The following discussion talks about the research contribution. A new parallel corpus for the English-Hindi was created  which consisted of parallel-aligned 81K pairs of sentences. The corpus consisted of human generated English sentences and their equivalent in Hindi. These sentences were obtained from the transcripts of the TED talks which depicted experiences of different speakers in different fields of life. 

A new baseline neural translation model was created using the existing technologies which performed better than the existing machine models that were presented in leading machine translation workshops, though the baseline model for this research was trained with very limited resource.

This work presented a novel approach that can be applied in the adaptation of the back-translation method for different low-resource language pairs. 

The research illustrates that domain specific training of a neural translation model with cross-domain specific training data leads to domain adaptation. The domain specific fine-tuning data, which consisted transcripts from TED Talks, was used to fine-tune the translation model. It was tested with the test data consisting of sentences extracted from travel blogs. The results of the experiments indicated that a cross-domain trained neural translation model performs well on the in-domain data, and subsequently generates above average quality translations. In a broader perspective this research bridges the identified gap, of a lacking English to Hindi focus, in the State of Art of Machine Translation by creating a new baseline translation model for large English-Hindi corpus leading to better than average results.

\section{Future Work}
There are energizing opportunities for future work, potential expansions to the research attempted in this thesis. This research goes about as an exciting step towards building domain specific neural translation models for language translations, that can be used for creating more language specific digital content. This work presents an exciting opportunity for  a broad research in the neural machine translation for English-Hindi language pairs, where research ought to be continued.

\subsection{Implementing Transformer Model}
The work of \cite{DBLP:journals/corr/VaswaniSPUJGKP17} proposed a new network architecture for neural machine translation called the Transformer which was solely based on attention mechanisms. The implementation of Transformer network has shown noteworthy improvements in BLEU scores for different Language pairs. A part of future work would involve a experiments to observe how the Transformer network performs on a large English-Hindi corpus. If the performance of transformer model is better than the baseline model used in this research, the baseline model would be replaced by the Transformer model for future research. 

\subsection{Training on high end GPU System}
This research work was restricted due to unavailability of high performing GPUs. The neural translation models could not be trained for optimal number of steps/epochs which may have improved the translation qualities by a significant margins. The future work would include re-training of the baseline model for 1 million steps.Then would involve generation of around 2.5 million synthetic sentences for the re-training of the baseline model. This would lead the translation model perform better than the existing models, improving the quality of translations. 
\subsection{Human Evaluation}

During the evaluation phase, when the BLEU scores and the translation quality of the translated sentences were compared it highlighted that the BLEU scores doesn't actually represent good quality translation. A good BLEU score doesn't guarantee a great quality translation or a bad BLEU score doesn't guarantee a bad quality translation. So the experiments suggest there is a need for the formulation of a better translation metric.

As part of future work, it would be beneficial if a human evaluation method is devised to judge the quality of translation. It could be typically a rating website which have the text in English, reference text in Hindi and translated text in Hindi. The ratings could be from the range of zero to five, with zero being a \textit{nonsense translation} while five being a \textit{perfect translation}. And then the human evaluation can be related with the BLEU scores to judge the quality of translation.  

\subsection{Web Application}
As well as research opportunities, this work also have a social relevance. There are millions of digitally neglected people in India who do not get any digital content in their mother language, or even in the most common language Hindi. The most of the digital content available in India are English, where as a significant number of population do not know English. If these travel content are available in Hindi for these people it will create a new social experience for them and would boost the tourism economy of the country indirectly.  This work provided an initial step by creating a visual interface to visualize the blog summaries in Hindi language.

This work could be further extended by creating a real time dynamic website which utilize the trained translation model in the back-end to create translations in real time. The website could be built on the lines of Kaffeehouse.com\footnote{\url{https:/www.Kaffeehouse.com},accessed:21.08.2018} which is a crowd-sourced Poetry Translation Platform co-founded by Kevin Koidl. But instead of crowd sourcing,the real time website could scrap travel related content from publicly available websites and dynamically populate their summaries in Hindi, and would translate the entire texts of the content in real time if the user is interested in reading them. This would enable a lot people to have access to digital travel content which would be great initiative towards digital literacy. 